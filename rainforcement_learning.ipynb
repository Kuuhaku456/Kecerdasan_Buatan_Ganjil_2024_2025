{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8D9yCVnBUaK",
        "outputId": "07ca719b-a8a1-4cea-db69-c920db98e0b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Buat environment FrozenLake\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "# Parameter Q-Learning\n",
        "action_size = env.action_space.n   # Banyaknya aksi yang bisa diambil\n",
        "state_size = env.observation_space.n  # Banyaknya state yang mungkin\n",
        "q_table = np.zeros((state_size, action_size))  # Inisialisasi Q-table\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.8   # alpha\n",
        "discount_rate = 0.95  # gamma\n",
        "epsilon = 1.0         # Untuk epsilon-greedy policy\n",
        "epsilon_min = 0.01    # Nilai minimum epsilon\n",
        "epsilon_decay = 0.995 # Untuk mengurangi epsilon secara bertahap\n",
        "episodes = 10000      # Jumlah episode latihan\n",
        "max_steps = 100       # Langkah maksimal per episode\n",
        "\n",
        "# Latih agent menggunakan Q-Learning\n",
        "for episode in range(episodes):\n",
        "    state = env.reset()  # Reset environment di awal episode\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Pilih action (epsilon-greedy strategy)\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Random action (exploration)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])  # Pilih action terbaik (exploitation)\n",
        "\n",
        "        # Lakukan action, dan dapatkan feedback (reward, state berikutnya, dan status selesai)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q-table berdasarkan rumus Q-Learning\n",
        "        q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_rate * np.max(q_table[next_state, :]) - q_table[state, action])\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Update nilai epsilon untuk lebih banyak exploitation di episode selanjutnya\n",
        "    if epsilon > epsilon_min:\n",
        "        epsilon *= epsilon_decay\n",
        "\n",
        "# Tampilkan Q-Table hasil training\n",
        "print(\"Q-Table setelah latihan:\")\n",
        "print(q_table)\n",
        "\n",
        "# Evaluasi hasil setelah latihan\n",
        "total_rewards = 0\n",
        "\n",
        "for episode in range(100):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        action = np.argmax(q_table[state, :])  # Pilih action terbaik berdasarkan Q-Table\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        total_rewards += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "print(f\"Total reward yang didapat setelah evaluasi: {total_rewards}\")\n",
        "env.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E4anLqywBkey",
        "outputId": "11fb859e-312a-42ca-c21d-90c35bf5a8ea"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-Table setelah latihan:\n",
            "[[1.80815631e-01 6.02380502e-02 4.58565037e-02 1.94080776e-02]\n",
            " [1.53371939e-03 1.04811473e-02 3.92776990e-05 1.03560110e-01]\n",
            " [1.36699239e-01 2.67487223e-02 7.39681381e-03 3.04344732e-02]\n",
            " [6.40975688e-04 2.21085549e-04 2.29833106e-03 3.19803545e-02]\n",
            " [1.61676527e-01 4.01300994e-02 4.99960011e-04 3.51528742e-02]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [3.77970352e-02 3.12514334e-04 3.10698117e-04 4.58583250e-06]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [8.93447722e-02 1.49066161e-02 4.16345237e-02 3.01222012e-01]\n",
            " [6.47890774e-02 3.93735804e-01 3.04252731e-02 2.87796027e-02]\n",
            " [2.14778891e-01 3.29568493e-03 4.90512218e-03 4.38343937e-03]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.04354997e-02 6.92300979e-03 6.43922995e-01 9.72392802e-02]\n",
            " [1.94752047e-01 9.47849864e-01 2.69408268e-01 2.12100225e-01]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
            "Total reward yang didapat setelah evaluasi: 82.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E4Hi9D9VBpdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}